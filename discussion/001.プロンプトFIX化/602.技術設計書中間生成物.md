<document-tec_result>
1. はじめに

1.1. 目的

本設計書は、人事異動リファレンスデータ管理のバッチシステムの技術設計について詳細を定義し、開発者および関係者間で設計内容を共有することを目的とする。本設計書に基づき、システム開発を効率的かつ品質高く進めることを目指す。

1.2. 範囲

本設計書の範囲は、人事異動リファレンスデータ管理のバッチシステムの全機能を対象とする。対象システムは、行内の人事異動情報を一括で取り込み、各種編集処理を行い、他システムへのデータ連携を行う。

1.3. 用語定義

本設計書で使用する主な用語について以下の通り定義する。

| 用語 | 説明 |
|------|------|
| リファレンスデータ | 人事異動情報を管理するマスタデータ |
| 編集処理 | リファレンスデータの編集・加工処理 |
| データ連携 | 編集処理後のデータを他システムに引き渡すこと |
| ディシジョンテーブル | 編集処理の適用ルールを定義したテーブル |

2. システムアーキテクチャ

2.1. 処理フロー

本システムの処理フローは以下の通り。

1. 人事データ取り込み
   - 人事部門から提供されるCSVファイルを取り込む
2. 一括申請処理
   - 取り込んだデータに対し、フォーマット変換や整合性チェックを行う
3. 受付処理
   - 一括申請処理後のデータに対し、データクレンジングや補完を行う
   - ディシジョンテーブルを用いて、適用する編集処理を決定する
4. パターン編集処理
   - 受付処理で決定した編集処理を適用し、リファレンスデータを生成する
   - ディシジョンテーブルを用いて、適用する編集パターンを決定する
5. 連携ファイル作成
   - リファレンスデータを他システムに連携するためのファイルに出力する
6. ファイル転送
   - 連携ファイルを他システムに転送する

本フローは、バッチ処理として一括で実行される。処理の実行制御はJenkinsで行う。

2.2. モジュール構成

本システムは、以下のPythonモジュールで構成される。

| モジュール | 説明 |
|------------|------|
| common | 共通処理を提供するモジュール |
| input | 人事データの取り込み処理を行うモジュール |
| apply | 一括申請処理を行うモジュール |
| accept | 受付処理を行うモジュール |
| edit | パターン編集処理を行うモジュール |  
| output | 連携ファイルの作成処理を行うモジュール |
| transfer | 連携ファイルの転送処理を行うモジュール |
| main | バッチ処理の制御フローを実装するモジュール |

各モジュールは、単一責任の原則に則り、独立性を保つよう設計する。モジュール間の依存関係は、なるべく疎結合になるよう留意する。

2.3. データフロー

本システムの処理フローに沿って、以下のデータフローで処理が行われる。

1. CSVファイルの読み込み (inputモジュール)
   - 人事部門から提供されるCSVファイルを読み込み、DataFrameに変換する
   - 読み込んだDataFrameは、pickleでファイル保存する
2. 一括申請処理 (applyモジュール)
   - inputモジュールが保存したDataFrameを読み込む 
   - フォーマット変換や整合性チェックを行い、結果をDataFrameに反映する
   - 処理結果のDataFrameは、pickleでファイル保存する
3. 受付処理 (acceptモジュール)  
   - applyモジュールが保存したDataFrameを読み込む
   - データクレンジングや補完を行い、結果をDataFrameに反映する
   - ディシジョンテーブルを用いて、適用する編集処理を決定する
   - 処理結果のDataFrameは、pickleでファイル保存する
4. パターン編集処理 (editモジュール)
   - acceptモジュールが保存したDataFrameを読み込む
   - ディシジョンテーブルを用いて、適用する編集パターンを決定する  
   - 編集パターンに従って、DataFrameを編集する
   - 処理結果のDataFrameは、pickleでファイル保存する
5. 連携ファイル作成 (outputモジュール)
   - editモジュールが保存したDataFrameを読み込む
   - DataFrameから連携項目を抽出し、CSVファイルに出力する
6. ファイル転送 (transferモジュール)
   - outputモジュールが作成したCSVファイルを、SSHを使って他システムに転送する

上記のデータフローに従い、各モジュールはデータの読み込みと書き出しを行う。モジュール間のデータ受け渡しは、ファイルを介して行う。

2.4. 永続化方式

本システムでは、各処理フェーズでデータの永続化を行う。永続化はpickleモジュールを用いて行う。
以下の機能で、DataFrameのpickle化・非pickle化を行う。

- pickle化
  - DataFrameをpickle形式でファイル保存する
  - ファイル名は、処理フェーズ名とタイムスタンプを組み合わせる
- 非pickle化  
  - pickle形式のファイルを読み込み、DataFrameに変換する
  - ファイル名は、処理フェーズ名とタイムスタンプを指定する

pickleファイルの保存先は、設定ファイルで指定する。保存先は、処理フェーズ毎に分けて管理する。
各フェーズの処理が途中で失敗した場合、そのフェーズまでのpickleファイルを使って再実行できるようにする。

3. 詳細設計

3.1. common

共通処理を提供するモジュール。

3.1.1. 全体説明

本モジュールは、以下の共通処理を提供する。

- ロギング
  - ログ出力処理を提供する
  - ログレベルは、設定ファイルで指定する
  - ログフォーマットは、「日時 ログレベル モジュール名 関数名 メッセージ」とする
- 設定ファイル読み込み
  - 設定ファイル(config.ini)を読み込み、Configオブジェクトを生成する
  - Configオブジェクトは、他のモジュールからimportして使用する
- 例外処理
  - 独自の例外クラスを定義する
  - 例外発生時は、ログ出力とともに、独自例外をraiseする

3.1.2. 処理定義

| 処理 | 説明 |
|------|------|
| get_logger | ロガーを取得する | 
| read_config | 設定ファイルを読み込む |
| write_config | 設定ファイルを書き出す |
| AppException | アプリケーション例外クラス |
| DataException | データ不正例外クラス |
| SystemException | システム例外クラス | 

3.1.3. 処理説明

- get_logger
  - 指定された名前のロガーを取得する
  - ロガーが存在しない場合は、新規に作成する
  - ロガーの設定は、ロギングの設定ファイル(logging.conf)で行う
- read_config
  - 指定された設定ファイルを読み込み、Configオブジェクトを生成する
  - 設定ファイルが存在しない場合は、FileNotFoundErrorをraiseする
- write_config  
  - 指定されたConfigオブジェクトを、設定ファイルに書き出す
  - 設定ファイルが存在しない場合は、新規に作成する
- AppException
  - アプリケーション例外の基底クラス
  - 他の例外クラスは、このクラスを継承して定義する
- DataException
  - データ不正例外のクラス
  - 入力データの不正や、マスタデータとの不整合などを表す
- SystemException
  - システム例外のクラス 
  - ファイルI/Oエラーや、外部システムとの通信エラーなどを表す

3.1.4. 資料

- 別紙「設定ファイル仕様書」
- 別紙「ロギング設定ファイル仕様書」

3.2. input

人事データの取り込み処理を行うモジュール。

3.2.1. 全体説明

本モジュールは、人事部門から提供されるCSVファイルを取り込み、DataFrameに変換する処理を行う。
CSVファイルは、FTPで受信する。FTP受信後、ファイルの形式や項目数のチェックを行い、不正がある場合はデータ不正例外を送出する。
チェックにパスしたCSVファイルは、DataFrameに変換し、pickle形式で保存する。

3.2.2. 処理定義

| 処理 | 説明 |
|------|------|
| receive_file | 人事データCSVファイルをFTPで受信する |
| validate_file | 受信したCSVファイルの形式をチェックする |  
| load_csv | CSVファイルを読み込み、DataFrameに変換する |
| save_dataframe | DataFrameをpickle形式で保存する |

3.2.3. 処理説明

- receive_file
  - FTPクライアントを使って、人事データCSVファイルを受信する
  - FTPの接続情報は、設定ファイルから取得する
  - 受信したファイルは、一時ディレクトリに保存する
- validate_file
  - 受信したCSVファイルが、定義された形式に合致しているかチェックする
  - ファイル名のフォーマット、ヘッダ行の有無、カラム数などをチェックする
  - 形式不正の場合は、DataExceptionをraiseする  
- load_csv
  - validate_fileでチェックしたCSVファイルを、DataFrameに読み込む
  - 読み込み時のデータ型は、設定ファイルで定義する
  - 読み込みに失敗した場合は、DataExceptionをraiseする
- save_dataframe
  - load_csvで読み込んだDataFrameを、pickle形式で保存する
  - 保存先ディレクトリは、設定ファイルで指定する
  - 保存先ディレクトリが存在しない場合は、ディレクトリを作成する

3.2.4. 資料

- 別紙「人事データCSVファイルレイアウト」

3.3. apply

一括申請処理を行うモジュール。

3.3.1. 全体説明

本モジュールは、inputモジュールが読み込んだDataFrameに対して、以下の処理を行う。

- フォーマット変換
  - 各列のデータ型を変換する
  - 日付文字列をdatetime型に変換する
  - コード値の表記ゆれを吸収する
- 整合性チェック  
  - 必須項目が入力されているかチェックする
  - コード値が定義された値の範囲内かチェックする
  - 従属関係にある項目の整合性をチェックする

フォーマット変換と整合性チェックにパスしたDataFrameは、pickle形式で保存する。
チェックでエラーが見つかった場合は、当該レコードを除外したDataFrameを保存する。
除外されたレコードは、エラーファイルに出力する。

3.3.2. 処理定義

| 処理 | 説明 | 
|------|------|
| load_dataframe | pickle形式のDataFrameを読み込む |
| convert_format | DataFrameのフォーマットを変換する |
| validate_data | DataFrameの整合性をチェックする |
| save_dataframe | DataFrameをpickle形式で保存する |
| output_error | エラーレコードをファイル出力する |

3.3.3. 処理説明

- load_dataframe
  - inputモジュールが保存したpickle形式のDataFrameを読み込む
- convert_format
  - DataFrameの各列に対し、以下のフォーマット変換を行う
    - 数値型の列は、float型に変換する
    - 日付文字列の列は、datetime型に変換する
    - コード値の表記ゆれを吸収する (例: "01" → "1")
- validate_data  
  - 以下の整合性チェックを行う
    - 必須項目のNULLチェック
    - コード値の値域チェック
    - 従属関係にある項目の整合性チェック (例: 所属部署と役職の整合性)
  - チェックに違反したレコードは、DataFrameから除外する
- save_dataframe
  - チェックに合格したレコードを持つDataFrameを、pickle形式で保存する
  - 保存先ディレクトリは、設定ファイルで指定する
- output_error
  - チェックに違反し、DataFrameから除外されたレコードを、CSVファイルに出力する
  - 出力先ディレクトリは、設定ファイルで指定する
  - 出力ファイル名は、処理日時をファイル名に含める (例: error_20230401_123456.csv)

3.3.4. 資料

- 別紙「フォーマット変換仕様書」
- 別紙「整合性チェック仕様書」
- 別紙「エラーファイルレイアウト」

3.4. accept

受付処理を行うモジュール。

3.4.1. 全体説明

本モジュールは、applyモジュールが処理したDataFrameに対して、以下の処理を行う。

- データクレンジング
  - 文字列の前後の空白を除去する
  - 全角文字を半角文字に変換する
  - 区切り文字の統一を行う
- データ補完
  - マスタデータを参照し、名称などの補完を行う
  - ビジネスルールに基づき、デフォルト値を設定する
- 編集処理の決定
  - ディシジョンテーブルを参照し、各レコードに適用する編集処理を決定する
  - ディシジョンテーブルには、適用条件と編集処理の組み合わせを定義する

処理後のDataFrameは、pickle形式で保存する。
次フェーズで実行する編集処理の情報は、DataFrameの列に追加する。

3.4.2. 処理定義

| 処理 | 説明 |
|------|------|
| load_dataframe | pickle形式のDataFrameを読み込む |
| cleanse_data | DataFrameのデータをクレンジングする |
| complement_data | DataFrameのデータを補完する |
| decide_edit_process | 適用する編集処理を決定する |
| save_dataframe | DataFrameをpickle形式で保存する |

3.4.3. 処理説明

- load_dataframe
  - applyモジュールが保存したpickle形式のDataFrameを読み込む
- cleanse_data
  - DataFrameの文字列列に対し、以下のクレンジング処理を行う
    - 前後の空白を除去する (strip)
    - 全角英数記号を半角に変換する (normalize)
    - 区切り文字をカンマに統一する
- complement_data
  - マスタデータを参照し、以下の補完処理を行う
    - 部署コードから部署名を設定する
    - 役職コードから役職名を設定する
  - ビジネスルールに基づき、以下の補完処理を行う
    - 所属年数が空欄の場合、「1年目」を設定する
    - 役職が空欄の場合、「一般職」を設定する
- decide_edit_process
  - ディシジョンテーブルを参照し、各レコードの編集処理を決定する
  - ディシジョンテーブルは、CSVファイルから読み込む
  - ディシジョンテーブルの適用条件は、DataFrameの列値で評価する
  - 決定した編集処理は、DataFrameの「edit_process」列に設定する
- save_dataframe  
  - 処理後のDataFrameを、pickle形式で保存する
  - 保存先ディレクトリは、設定ファイルで指定する

3.4.4. 資料

- 別紙「マスタデータ一覧」
- 別紙「ディシジョンテーブル仕様書」

3.5. edit

パターン編集処理を行うモジュール。

3.5.1. 全体説明

本モジュールは、acceptモジュールが処理したDataFrameに対して、レコード毎に編集処理を実行する。
実行する編集処理は、DataFrameの「edit_process」列に設定されている。
編集処理は、以下の3つのフェーズに分かれる。

1. 事前処理
   - 編集処理に必要な情報を準備する
   - マスタデータの読み込み、編集対象列の抽出などを行う
2. 編集処理
   - レコードに対して、編集処理を実行する
   - 編集処理は、編集パターンに従って実行される
   - 編集パターンは、ディシジョンテーブルで定義する
3. 事後処理
   - 編集処理の結果を確認する
   - 編集結果の妥当性チェック、エラーハンドリングなどを行う

編集処理後のDataFrameは、pickle形式で保存する。

3.5.2. 処理定義

| 処理 | 説明 |
|------|------|
| load_dataframe | pickle形式のDataFrameを読み込む |  
| preprocess | 編集処理の事前処理を行う |
| execute_edit | 編集処理を実行する |
| postprocess | 編集処理の事後処理を行う |
| save_dataframe | DataFrameをpickle形式で保存する |

3.5.3. 処理説明

- load_dataframe
  - acceptモジュールが保存したpickle形式のDataFrameを読み込む
- preprocess
  - 編集処理に必要なマスタデータをデータベースから読み込む
  - DataFrameから、編集対象の列を抽出する
- execute_edit
  - DataFrameの各レコードに対し、以下の処理を行う
    1. レコードの「edit_process」列で指定された編集処理を特定する
    2. 編集処理に対応する編集パターンを、ディシジョンテーブルから取得する
    3. 編集パターンに従い、レコードの編集を行う
  - 編集パターンは、正規表現による文字列置換や、マスタデータを用いた値の変換などを定義する
- postprocess  
  - 編集処理の結果を確認する
  - 編集後のデータに対し、妥当性チェックを行う
  - 編集処理でエラーが発生した場合、ログ出力を行う
- save_dataframe
  - 編集処理後のDataFrameを、pickle形式で保存する
  - 保存先ディレクトリは、設定ファイルで指定する

3.5.4. 資料

- 別紙「編集パターン定義書」
- 別紙「編集処理エラーコード一覧」

3.6. output

連携ファイルの作成処理を行うモジュール。

3.6.1. 全体説明

本モジュールは、editモジュールが処理したDataFrameから、連携ファイルを作成する。
連携ファイルは、他システムとのデータ連携に使用するCSVファイルである。
連携ファイルへの出力項目は、設定ファイルで定義する。
出力対象のレコードは、ビジネスルールに基づいてDataFrameから抽出する。

3.6.2. 処理定義

| 処理 | 説明 |
|------|------|
| load_dataframe | pickle形式のDataFrameを読み込む |
| filter_records | 出力対象レコードを抽出する |
| generate_csv | CSVファイルを生成する |

3.6.3. 処理説明

- load_dataframe
  - editモジュールが保存したpickle形式のDataFrameを読み込む
- filter_records  
  - ビジネスルールに基づき、出力対象のレコードを抽出する
  - 抽出条件は、DataFrameの列値に対する条件式で定義する
  - 抽出条件は、設定ファイルで指定する
- generate_csv
  - 抽出したレコードを、CSVファイルに出力する
  - 出力項目は、設定ファイルで指定する
  - 出力先ディレクトリは、設定ファイルで指定する
  - ファイル名は、「連携先システム名_YYYYMMDD.csv」の形式とする

3.6.4. 資料

- 別紙「連携ファイル仕様書」
- 別紙「連携ファイル出力項目定義書」

3.7. transfer

連携ファイルの転送処理を行うモジュール。

3.7.1. 全体説明

本モジュールは、outputモジュールが生成した連携ファイルを、他システムに転送する。
転送先は複数存在し、転送方式はSFTPを使用する。
転送先毎に、転送するファイルと転送先ディレクトリを設定ファイルで定義する。

3.7.2. 処理定義

| 処理 | 説明 |
|------|------|
| load_config | 設定ファイルを読み込む |
| list_files | 転送対象ファイルを取得する |
| transfer_files | ファイルを転送する |

3.7.3. 処理説明 

- load_config
  - 設定ファイルを読み込み、転送設定情報を取得する
  - 転送設定情報には、転送先ごとに以下の情報が定義されている
    - 転送先システム名
    - 転送先ホスト名
    - 転送先ユーザ名
    - 転送先パスワード  
    - 転送先ディレクトリ
    - 転送対象ファイル名のパターン
- list_files
  - outputモジュールが生成した連携ファイルの一覧を取得する
  - 設定ファイルで定義した転送対象ファイル名のパターンに合致するファイルを抽出する
- transfer_files
  - list_filesで抽出したファイルを、設定ファイルで定義した転送先に転送する
  - 転送はSFTPを使用する
  - 転送先ディレクトリが存在しない場合は、ディレクトリを作成する
  - ファイルの転送が成功した場合、転送元のファイルを削除する
  - 転送処理でエラーが発生した場合、ログ出力を行う

3.7.4. 資料

- 別紙「SFTP転送設定書」
- 別紙「転送エラーコード一覧」  

4. 非機能要件

4.1. ログ出力

アプリケーションログの出力先は以下の通り。

- ローカル環境
  - ログはコンソール出力する
- 開発環境・本番環境
  - ログはファイル出力する
  - ログファイルのローテーションは、1日1ファイルとする
  - ローテーション世代数は、30世代とする

ログレベルはDEBUG/INFO/WARNING/ERROR/CRITICALの5段階とする。
本番環境ではINFO以上、開発環境ではDEBUG以上を出力する。
ログフォーマットは以下の通りとする。

- 日時 ログレベル プロセスID モジュール名 関数名 メッセージ

4.2. テスト

以下の方針でテストを実施する。

- ユニットテスト
  - 各モジュールの単体テストを行う
  - テストフレームワークとしてpytestを使用する
  - テストカバレッジは80%以上を目標とする
- 結合テスト
  - モジュール間の連携処理に対するテストを行う
  - テストデータは、過去の運用で使用した実データを使用する
  - 結合テストは、開発環境で実施する
- システムテスト
  - システム全体の機能を網羅したテストを行う
  - テストシナリオは、業務シナリオに沿って作成する
  - システムテストは、本番環境と同等の環境で実施する

テストの自動化にはJenkinsを使用する。
単体テスト・結合テストは、Gitへのプッシュをトリガーとして自動実行する。
システムテストは、リリース前に手動でジョブを起動し、実行する。

4.3. パフォーマンス

本システムはバッチ処理であり、オンライン性能は求められない。
十分なパフォーマンスを確保するため、以下の施策を行う。

- DataFrameの処理はメモリ上で行い、中間ファイル出力は必要最小限とする
- 大量データの処理は、chunksizeを指定してバッチ処理化する
- CPUバウンドな処理は、multiprocessingを使って並列処理化する
- ファイル転送は、並列化・マルチスレッド化する

5. 設計書一覧

- 別紙「設定ファイル仕様書」
- 別紙「ロギング設定ファイル仕様書」
- 別紙「人事データCSVファイルレイアウト」
- 別紙「フォーマット変換仕様書」
- 別紙「整合性チェック仕様書」
- 別紙「エラーファイルレイアウト」
- 別紙「マスタデータ一覧」
- 別紙「ディシジョンテーブル仕様書」
- 別紙「編集パターン定義書」
- 別紙「編集処理エラーコード一覧」
- 別紙「連携ファイル仕様書」
- 別紙「連携ファイル出力項目定義書」
- 別紙「SFTP転送設定書」
- 別紙「転送エラーコード一覧」


6. 移行計画

6.1. 移行方針

本システムは、既存の人事データ管理業務を刷新するものであるため、業務移行が必要となる。
移行方針は以下の通り。

- 人事データの移行は、一括移行方式で実施する
- 移行対象は、現行の人事データベースに登録されている全データとする
- 移行は、本番稼働の3ヶ月前から開始し、本番稼働の1ヶ月前には完了させる
- 現行システムと新システムの並行稼働期間は設けない
- 移行リハーサルを2回実施し、移行手順の検証と修正を行う

6.2. 移行スケジュール

移行スケジュールは以下の通り。

| 時期 | 作業内容 |
|------|------|
| 本番稼働3ヶ月前 | 移行計画の策定<br>移行ツールの開発 |
| 本番稼働2ヶ月前 | 第1回移行リハーサルの実施<br>移行手順の修正 |
| 本番稼働1ヶ月前 | 第2回移行リハーサルの実施<br>移行手順の確定 |
| 本番稼働1週間前 | 移行判定会議の実施<br>移行Go/Nogoの決定 |
| 本番稼働日 | 移行作業の実施 |

6.3. 移行体制

移行体制は以下の通り。

| 役割 | 担当 |
|------|------|
| 移行責任者 | プロジェクトマネージャー |
| 移行実施者 | 開発チーム |
| 移行監視者 | 業務部門の主要メンバー |
| 移行技術支援者 | インフラ部門 | 

7. 運用計画

7.1. 運用体制

本システムの運用体制は以下の通り。

| 役割 | 担当 |
|------|------|
| システム管理者 | インフラ部門 |
| アプリケーション管理者 | 開発部門 |
| ユーザサポート | ヘルプデスク |
| モニタリング | インフラ部門 |

7.2. 運用監視

以下の項目についてモニタリングを行う。

- ジョブの実行状況
  - ジョブスケジューラのログを監視し、ジョブの実行状況を確認する
  - ジョブが異常終了した場合、アラートを発報する
- ログのエラー
  - アプリケーションログをモニタリングし、エラーログの発生を検知する
  - エラーログが発生した場合、アラートを発報する
- リソース使用状況  
  - サーバのCPU、メモリ、ディスク使用率を監視する
  - 閾値を超過した場合、アラートを発報する

監視は、Zabbixを使用して行う。
監視対象のサーバにZabbix Agentをインストールし、監視サーバへデータを送信する。
監視サーバでは、Zabbix Serverが稼働し、監視データを収集・蓄積する。
収集したデータは、Zabbix Frontendで可視化する。

7.3. 保守計画

本システムでは、以下の保守を実施する。

- 定期保守
  - 月1回、サーバのOSとミドルウェアのアップデートを実施する
  - アップデートは、保守用の時間帯に実施する
  - アップデートの適用前後で、動作確認を行う
- パッチ適用
  - セキュリティパッチなど、緊急性の高いパッチは随時適用する
  - パッチ適用前に、テスト環境で動作検証を行う
- 不具合対応
  - 不具合発生時は、障害レベルに応じて対応する
  - 障害レベルは、ビジネスへの影響度に応じて設定する
  - 上位の障害レベルには、SLA違反のペナルティを設定する

7.4. 提供サービス

本システムでは、以下のサービスを業務部門に提供する。

- 人事データの登録・更新
  - 人事データの登録・更新を行う画面を提供する
  - 登録・更新されたデータは、リアルタイムで他システムに連携される
- 帳票出力
  - 人事データを帳票形式で出力する機能を提供する  
  - 出力帳票の種類は、設計工程で定義する
- データ抽出
  - 人事データの抽出機能を提供する
  - 抽出条件は、業務部門が自由に設定できる
  - 抽出したデータは、CSVファイルでダウンロードできる

8. 追記事項

- 本設計書は、現時点での設計内容をまとめたものである
- 今後の要件変更や設計の詳細化に伴い、内容が変更される可能性がある
- 変更が発生した場合は、変更履歴を記録し、関係者に通知する
- 本設計書の変更は、プロジェクトマネージャーの承認を得て行う

以上。
</document-tec_result>
